model:
  in_channel: 1
  K: 6 # 2, 3, 4a, 4b, 4c, 5
  width: 16

  # for (512, 512) image, unet level + ypath level <= 8
  ulevel: 3
  ylevel: 2 

  # margin when calculating triplet loss
  margin: 0.3

  # use torch checkpoint to save memory on training
  memory_trade: True

# coefficients when calculating multi-task loss
coefficients:
  task_tm: 0.05
  task_seg: 1 - 0.2 * (x ** 0.5)

datasets:
  - set2
  - set3
  - BUSI

# trainer flag
flag:
  gpus: 1
  # precision: 16
  # amp_backend: apex
  min_epochs: 40
  max_epochs: 80
  num_sanity_val_steps: 0
  log_every_n_steps: 10
  terminate_on_nan: True

misc:
  augment: 1200
  continue: False
  load_from: latest
  use_annotation_from: 0

optimizer:
  - AdamW
  - lr: 2.5e-3

# LRReduceOnPlateau only
scheduler: null

# branch is used to control behaviors of diverse param groups.
# args here will override those in optimizer and scheduler
branch:
  M:
    optimizer:
      weight_decay: 1.0e-3
    scheduler:
      factor: 0.9
      patience: 4

dataloader:
  training:
    batch_size: 10
    shuffle: True

  # for evaluating, validating, testing
  validating:
    batch_size: 16

paths:
  name: ynet
  model_dir: model/toynetv1/ynet
  log_dir: log/toynetv1/{date}
  post_training: src/post_training.py
